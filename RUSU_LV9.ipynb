{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMO/wsveKkLGWnzbC+kR910",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferit-ml/RUSU-LV9/blob/main/RUSU_LV9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Podržano učenje\n",
        "\n",
        "Podržano učenje (*reinforcement learning*) je posebna vrsta strojnog učenja, različita od nadziranog i nenadziranog učenja. Za razliku od nadziranog i nenadziranog učenja, kod podržanog učenja nemamo podatkovni skup s ulaznim (i izlaznim) podacima, nego imamo nekakvo okruženje pomoću kojeg treniramo agenta.\n",
        "\n",
        "Agent se nalazi u nekakvom okruženju (*environment*) u kojem može obavljati određene akcije. U svakom trenutku je agent u određenom stanju. Nakon što agent obavi nekakvu akciju u određenom stanju, okruženje agenta stavlja u novo stanje i nagrađuje (ili kažnjava) ga za odrađenu akciju. U ovom LV-u ćemo promatrati slučaj u kojem je vrijeme diskretno, postoji ograničeni skup akcija i ograničeni skup stanja.\n",
        "\n",
        "Definiramo sljedeće izraze:\n",
        "- $t$ - vremenski trenutak\n",
        "- $a_t$ - akcija u trenutku $t$\n",
        "- $\\mathcal{A}$ - skup akcija\n",
        "- $s_t$ - stanje u trenutku $t$\n",
        "- $\\mathcal{S}$ - skup stanja\n",
        "- $r_t$ - nagrada u trenutku $t$\n",
        "- $a_{t+1}$ - akcija u trenutku $t+1$\n",
        "- $s_{t+1}$ - stanje u trenutku $t+1$\n",
        "- $s_{t+1}$ - nagrada u trenutku $t+1$\n",
        "\n",
        "Agent se u trenutku $t$ nalazi u stanju $s_t$ te obavlja akciju $a_t$. Okruženje ga zatim stavlja u stanje $s_{t+1}$ te mu daje nagradu $r_{t+1}$. Prikaz ovakvog sustava prikazan je na shemi.\n",
        "\n",
        "![reinforcement-learning.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "xADm5wBR8Niy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Q-learning*\n",
        "\n",
        "Q-learning je algoritam za učenje tzv. q-vrijednosti. q-vrijednost neke akcije $a$ u stanju $s$ je procjena nagrade akcije $a$ u stanju $s$. Ova procjena ne uključuje samo trenutnu nagradu nego i moguće buduće nagrade (kazne). Odnosno, možemo definirati funkciju Q:\n",
        "\n",
        "$$Q: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$$\n",
        "\n",
        "q-vrijednosti možemo promatrati kao tablicu gdje nam retci označavaju sva moguća stanja ($\\forall s \\in \\mathcal{S})$, a stupci sve moguće akcije ($\\forall a \\in \\mathcal{A}$).\n",
        "\n",
        "| -      | Action 1 ($a^1$) | Action 2 ($a^2$) | Action 3 ($a^3$) | Action 4 ($a^4$) |\n",
        "| ----------- | ----------- | ----------- | ----------- | ----------- |\n",
        "| State 1 ($s^1$) | Q($s^1, a^1$) | Q($s^1, a^2$) | Q($s^1, a^3$) | Q($s^1, a^4$) |\n",
        "| State 2 ($s^2$) | Q($s^2, a^1$) | Q($s^2, a^2$) | Q($s^2, a^3$) | Q($s^2, a^4$) |\n",
        "| ... | ... | ... | ... | ... |\n",
        "\n",
        "\n",
        "Prije svega inicijaliziramo q-vrijednosti na neku zadanu početnu vrijednost za sva stanja i sve akcije u tim stanjima. q-vrijednosti učimo na sljedeći način:\n",
        "\n",
        "1. Agent se u trenutku $t$ nalazi u stanju $s_t$. Odabire akciju $a_t$ koju će odraditi.\n",
        "2. Okruženje daje nagradu (ili kaznu) $r_{t+1}$ i stavlja agenta u stanje $s_{t+1}$\n",
        "3. $Q(s_t, a_t)$ se ažurira\n",
        "\n",
        "Ovaj proces ponavljamo dok agent ne dođe u neko konačno stanje, primjerice *smrt* u slučaju agenta u nekakvoj video igri. Ovo nazivamo epizodom te treniranje možemo vršiti proizvoljan broj epizoda."
      ],
      "metadata": {
        "id": "NhmpE8vOb__a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ažuriranje q-vrijednosti\n",
        "\n",
        "Posljednji korak u svakom koraku *Q-learninga* je ažurairanje q-vrijednosti. Ovo činimo pomoću Bellmanove jednadžbe:\n",
        "\n",
        "$$Q(s_t, a_t) \\leftarrow (1 - \\alpha)Q(s_t, a_t) + \\alpha(r_{t+1} + \\gamma \\max_a{Q(s_{t+1}, a)})$$\n",
        "\n",
        "Gdje su:\n",
        "- $\\alpha$ - stopa učenja (*learning rate*)\n",
        "- $\\gamma$ - faktor popusta (*discount factor*)\n",
        "- $r_{t+1}$ - nagrada dobivena prelaskom iz stanja $s_t$ u stanje $s_{t+1}$\n",
        "\n",
        "Tako se nova vrijednost $Q(s_t, a_t)$ definira preko tri faktora:\n",
        "- $(1 - \\alpha)Q(s_t, a_t)$ - trenutna vrijednost pomnožena s $(1 - \\alpha)$\n",
        "- $\\alpha r_{t+1}$ - nagrada koja se dobije u trenutku $t+1$ nakon što se u stanju $s_t$ učini akcija $a_t$ pomnožena s $\\alpha$\n",
        "- $\\alpha \\gamma \\max\\limits_a{Q(s_{t+1}, a)}$ - maksimalna q-vrijednost za stanje $s_{t+1}$ pomnožena s $\\alpha$ i $\\gamma$\n",
        "\n",
        "Što je $\\alpha$ veći, to će nove informacije više utjecati na novu q-vrijednost. $\\gamma$ definira koliko će buduće nagrade, odnosno q-vrijednosti, utjecati na trenutku q-vrijednost. Ako je $\\gamma = 0$, algoritam uopće neće uzimati buduće q-vrijednosti u obzir, odnosno bit će *kratkovidan*. Na rezultat *Q-learninga* također može utjecati i odabir inicijalnih q-vrijednosti."
      ],
      "metadata": {
        "id": "COrOqzKIoeVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Exploitation vs. exploration*\n",
        "\n",
        "Bitan dio *Q-learninga* je odabir akcije $a$ u stanju $s$. Očit način je jednostavan odabir akcije s najvećom q-vrijednošću u stanju $s$. Ovakav odabir akcije se naziva eksploatacija (*exploitation*), a takav algoritam bi onda bio pohlepan (*greedy*). Ovakav algoritam bi se potencijalno teško istrenirao jer bi u jednom trenutku isprobao *dobru* akciju i nikad više ne bi isprobao drugu akciju, a među drugim akcijama možda postoji još bolja akcija.\n",
        "\n",
        "Da bi se ovo izbjeglo, potrebno je uključiti i istraživanje (*exploration*) u algoritam. Jedan od načina da se ovo učini je da se doda vjerojatnost za odabir nasumične akcije, neovisno o q-vrijednostima. Ovaj pristup se naziva *$\\epsilon$-greedy* pristup i vjerojatnost za odabir nasumične akcije se označava s $\\epsilon$. Na ovaj način se može postići ravnoteža između eksploatacije i istraživanja.\n",
        "\n",
        "Teško je odrediti *ispravnu* vrijednost parametra $\\epsilon$, ali se u praksi često koriste vrijednosti oko $0.1$. Također je moguće prilagođavati vrijednost parametra $\\epsilon$ za vrijeme treniranja, pa primjerice početi s većom vrijednosti parametra $\\epsilon$ te ju s vremenom smanjivati i na kraju ju postaviti na 0 kako bi se algoritam *fine-tuneao* samo eksploatacijom."
      ],
      "metadata": {
        "id": "_OikneguuPns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nedostaci *Q-learninga*\n",
        "\n",
        "Postoje određeni problemi *Q-learninga*. Jedan od njih je već spomenut u potrebi ostaviravanja ravnoteže između eksploatacije i istraživanja.\n",
        "\n",
        "Također moguć problem je pristranost precjenjivanju (*overestimation bias*). Budući da su q-vrijednosti šumovite, uzimanjem maksimalne vrijednosti, dobit će precijenjena vrijednost. Uzmemo li primjerice diskretnu uniformnu razdiobu s vrijednostima 0 i 1, 2 puta izvučemo nasumičan broj iz razdiobe i odaberemo veći, dobit ćemo očekivanu vrijednost od $0.75$ za razliku od stvarne srednje vrijednosti te razdiobe koja je $0.5$. Jedan od načina rješavanja ovog problema je tzv. *double Q-learning*.\n",
        "\n",
        "Još jedan nedostatak *Q-learninga* je problem visoke dimenzionalnosti kada je moguć velik broj stanja ili akcija. Primjerice, ako nam je promatranje u obliku slike, vrlo je teško modelirati tablicu q-vrijednosti gdje su nam moguća stanja sve moguće vrijednosti promatrane slike. Čest način zaobilaženja ovog problema je upotrebom dubokog (*deep*) *Q-learninga*."
      ],
      "metadata": {
        "id": "t0JFAv5jy7nD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTvxEOcJsIqx"
      },
      "outputs": [],
      "source": [
        "## Importing useful libraries\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from imageio.v2 import imread\n",
        "%matplotlib inline\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from enum import Enum\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.animation as plt_animation\n",
        "from matplotlib import rc\n",
        "rc('animation', html='jshtml')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zmijica\n",
        "\n",
        "Za potrebe ove laboratorijske vježbe, napravljena je jednostavna video igra zmijica. Video igra ne može primati ulaze s računala, odnosno, može se igrati jedino preko poziva u kodu. Igra je implementirana u klasi `Snake` te su implementirane i pomoćna klasa `State` i [enumeracije](https://docs.python.org/3/library/enum.html) `Direction` i `Input`.\n",
        "\n",
        "`Direction` predstavlja moguće smjerove (i orijentacije) u koje zmijica može gledati, a oni su `UP, RIGHT, DOWN, LEFT`. Uz ovu enumeraciju, definiran je i *dictionary* `direction_dict` koji definira pomake (`[x, y]`) za svaki od smjerova (orijentacija).\n",
        "\n",
        "`Input` predstavlja moguće naredbe koje zmijica može primiti, a to su `CHILL` za nastavak pravocrtnog kretanja te `RIGHT` i `LEFT` za skretanje desno i lijevo za $90^\\circ$.\n",
        "\n",
        "Klasa `State` se koristi za praćenje stanja zmijice, a sadrži sljedeće atribute i funkcije:\n",
        "- `apple_front_back` - predstavlja je li jabuka ispred ili iza glave zmijice, ovisi o smjeru u kojem zmijica gleda i poprima sljedeće vrijednosti: (-1, jabuka je iza zmijice), (0, jabuka nije ni ispred ni iza zmijice), (1, jabuka je ispred zmijice)\n",
        "- `apple_left_right` - analogno `apple_front_back`, predstavlja je li jabuka lijevo ili desno od glave zmijice: (-1, jabuka je lijevo od zmijice), (0, jabuka nije ni lijevo ni desno od zmijice), (1, jabuka je desno od zmijice)\n",
        "- `danger` - lista koja predstavlja je li zmijica u neposrednoj opasnosti\n",
        "u sljedećim smjerovima `[ispred, desno, lijevo]`, 1 predstavlja da je neposredna opasnost u tom smjeru, a 0 da nije\n",
        "- `distance` - euklidska udaljenost između glave zmijice i jabuke\n",
        "- `dead` - je li zmijica mrtva\n",
        "- `ate` - je li zmijica upravo pojela jabuku\n",
        "- `get_state_str()` - ispis stanja zmijice bitan za q-vrijednosti, uključuje `apple_front_back`, `apple_left_right` i `danger`\n",
        "- `print_state()` - ispisuje stanje zmijice na čovjeku čitljiv način\n",
        "\n",
        "Također je dostupan i *dictionary* za boje."
      ],
      "metadata": {
        "id": "fGT9IPoJzCG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sama klasa `Snake` u konstruktoru prima jedan parametar `board_size` koji definira veličinu polja u kvadratićima. Polje je kvadratnog oblika. Uz pomoćne funkcije, definirane su i funkcije `input`, `get_state` i `draw`.\n",
        "\n",
        "Funkcija `input` prima jedan parametar klase `Input` te obavlja sve što se dogodi prilikom jednog koraka igre: rotira zmijicu ukoliko je potrebno, pomiče zmijicu, detektira je li zmijica preminula, je li zmijica pojela jabuku te, ako je, generira novu jabuku i produljuje zmijicu.\n",
        "\n",
        "Funkcija `get_state` vraća trenutno stanje zmije u obliku objekta klase `State`. Funkcija `draw` vraća *sliku* trenutnog stanja igre, odnosno `numpy` polje koje se onda može crtati pomoću `matplotlib` biblioteke. Glavu zmijice crta zelenom bojom, tijelo plavom, a jabuku crvenom bojom. Ukoliko zmijica ugine, glava joj postaje tamno zelena."
      ],
      "metadata": {
        "id": "5a9f1bY-wp5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enumeracije su u Pythonu poseban tip klasa u kojima definiramo neke atribute i njihove cjelobrojne vrijednsoti. Na ovaj način možemo osigurati jednostavnu pretvorbu između enumeracija i pripadnih cjelobrojnih vrijednosti.\n",
        "\n",
        "Možemo kreirati enumeraciju pomoću broja određene stavke. Primjerice, za kreiranje `DummyEnum.VAL0`, možemo pozvati `DummyEnum(0)`. Isto tako je moguće u suprotnom smjeru djelovati i jednostavno dobiti cjelobrojnu vrijednost određene stavke. Primjerice `DummyEnum.VAL0.value` će vratiti vrijednost `0`. Primjer rada s enumeracijom, prikazan je u sljedećoj ćeliji."
      ],
      "metadata": {
        "id": "tpJt7dn5wxJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyEnum(Enum):\n",
        "  VAL0 = 0\n",
        "  VAL1 = 1\n",
        "  VAL2 = 2\n",
        "\n",
        "example_enum = DummyEnum.VAL1\n",
        "\n",
        "print('enum:            ', example_enum)\n",
        "\n",
        "print('enum value:      ', example_enum.value)\n",
        "\n",
        "example_enum_2 = DummyEnum((example_enum.value + 1) % 3)\n",
        "\n",
        "print('enum incremented:', example_enum_2)"
      ],
      "metadata": {
        "id": "A8IcIn_8w32j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = {\n",
        "  'white': [1, 1, 1],\n",
        "  'blue': [0, 0, 1],\n",
        "  'green': [0, 1, 0],\n",
        "  'red': [1, 0, 0],\n",
        "  'yellow': [1, 1, 0],\n",
        "  'dark_green': [0, 0.4, 0.2]\n",
        "}\n",
        "\n",
        "class Direction(Enum):\n",
        "  UP = 0\n",
        "  RIGHT = 1\n",
        "  DOWN = 2\n",
        "  LEFT = 3\n",
        "\n",
        "direction_dict = {\n",
        "  0: [-1, 0], ## UP\n",
        "  1: [0, 1],  ## RIGHT\n",
        "  2: [1, 0],  ## DOWN\n",
        "  3: [0, -1]  ## LEFT\n",
        "}\n",
        "\n",
        "class Input(Enum):\n",
        "  CHILL = 0\n",
        "  TURN_RIGHT = 1\n",
        "  TURN_LEFT = 2\n",
        "\n",
        "class State():\n",
        "  def __init__(self, apple_front_back, apple_left_right, danger, distance, dead, ate):\n",
        "    ## -1 for back, 0 for neither, 1 for front\n",
        "    ## up, down => state_1; left, right => state_2\n",
        "    self.apple_front_back = apple_front_back\n",
        "    ## -1 for left, 0 for neither, 1 for right\n",
        "    ## up, down => state_2; left, right => state_1\n",
        "    self.apple_left_right = apple_left_right\n",
        "    ## 0 no danger, 1 danger; [front, right, left]\n",
        "    self.danger = danger\n",
        "    self.distance = distance\n",
        "    self.dead = dead\n",
        "    self.ate = ate\n",
        "\n",
        "  def get_state_str(self):\n",
        "    return str([self.apple_front_back, self.apple_left_right, *self.danger])\n",
        "\n",
        "  def print_state(self):\n",
        "    print('state_str:', self.get_state_str())\n",
        "    print('distance:', self.distance)\n",
        "    print('dead:', self.dead)\n",
        "    print('ate:', self.ate)\n",
        "\n",
        "def array_in_list(arr, lst):\n",
        "  if np.any(np.all(arr == lst, axis=1)):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "class SnakeGame():\n",
        "  def __init__(self, board_size=10):\n",
        "    if board_size < 10:\n",
        "      board_size = 10\n",
        "\n",
        "    self.board_size = board_size\n",
        "    self.score = 0\n",
        "    self.dead = False\n",
        "    self.ate = False\n",
        "\n",
        "    ## Let the snake start somewhere near middle\n",
        "    self.head = np.random.randint(board_size // 2 - 2, board_size // 2 + 2, 2)\n",
        "\n",
        "    self.direction = Direction.RIGHT\n",
        "    self.snake = [self.head - self.direction_delta() - self.direction_delta() - self.direction_delta(),\n",
        "                  self.head - self.direction_delta() - self.direction_delta(),\n",
        "                  self.head - self.direction_delta(), self.head]\n",
        "\n",
        "    self.generate_apple()\n",
        "\n",
        "  def direction_delta(self):\n",
        "    return direction_dict[self.direction.value]\n",
        "\n",
        "  def generate_apple(self):\n",
        "    self.apple = np.random.randint(0, self.board_size, 2)\n",
        "    if array_in_list(self.apple, self.snake):\n",
        "      self.generate_apple()\n",
        "\n",
        "  def is_danger(self, new_loc):\n",
        "    '''\n",
        "    if snake will hit itself or if snake will go out of bounds\n",
        "    new_loc: location to check for danger\n",
        "    '''\n",
        "    return (array_in_list(new_loc, self.snake) or\n",
        "      np.any((new_loc < 0) | (new_loc >= self.board_size)))\n",
        "\n",
        "  def input(self, input):\n",
        "    self.ate = False\n",
        "    if self.dead:\n",
        "      return\n",
        "    if input == Input.TURN_RIGHT:\n",
        "      self.direction = Direction((self.direction.value + 1) % 4)\n",
        "    elif input == Input.TURN_LEFT:\n",
        "      self.direction = Direction((self.direction.value - 1) % 4)\n",
        "\n",
        "    new_head = self.head + self.direction_delta()\n",
        "    if self.is_danger(new_head):\n",
        "      # death\n",
        "      self.dead = True\n",
        "      return\n",
        "\n",
        "    self.head = new_head\n",
        "    self.snake.append(self.head)\n",
        "    if np.array_equal(self.head, self.apple):\n",
        "      self.generate_apple()\n",
        "      self.score += 1\n",
        "      self.ate = True\n",
        "    else:\n",
        "      self.snake.pop(0)\n",
        "\n",
        "  def get_state(self):\n",
        "    x_a, y_a = self.apple\n",
        "    x_h, y_h = self.head\n",
        "    direction = self.direction_delta()\n",
        "    ## non-zero direction\n",
        "    direction_1 = np.nonzero(direction)[0][0]\n",
        "    if x_a == x_h:\n",
        "      state_1 = 0\n",
        "    else:\n",
        "      state_1 = direction[direction_1] * (1 if int(x_a > x_h) else -1)\n",
        "    if y_a == y_h:\n",
        "      state_2 = 0\n",
        "    else:\n",
        "      state_2 = direction[direction_1] * (1 if int(y_a < y_h) else -1) * (-1 if direction_1 else 1)\n",
        "    ## -1 for back, 0 for neither, 1 for front\n",
        "    ## up, down => state_1; left, right => state_2\n",
        "    apple_front_back = state_2 if direction_1 else state_1\n",
        "    ## -1 for left, 0 for neither, 1 for right\n",
        "    ## up, down => state_2; left, right => state_1\n",
        "    apple_left_right = state_1 if direction_1 else state_2\n",
        "    ## 0 no danger, 1 danger; [front, right, left]\n",
        "    front = self.head + direction\n",
        "    right = self.head + direction_dict[(self.direction.value + 1) % 4]\n",
        "    left = self.head + direction_dict[(self.direction.value - 1) % 4]\n",
        "    danger = [int(self.is_danger(front)),\n",
        "              int(self.is_danger(right)),\n",
        "              int(self.is_danger(left))]\n",
        "\n",
        "    distance = np.linalg.norm(self.head - self.apple)\n",
        "\n",
        "    return State(apple_front_back, apple_left_right, danger, distance, self.dead, self.ate)\n",
        "\n",
        "  def draw(self):\n",
        "    self.board = np.ones((self.board_size, self.board_size, 3))\n",
        "    self.board[tuple(self.head)] = colors['dark_green'] if self.dead else colors['green']\n",
        "    for cell in self.snake[:-1]:\n",
        "      self.board[tuple(cell)] = colors['blue']\n",
        "    self.board[tuple(self.apple)] = colors['red']\n",
        "    return self.board.copy()"
      ],
      "metadata": {
        "id": "nZRxk00zQFTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Play():\n",
        "  def __init__(self, board_size=10):\n",
        "\n",
        "    '''\n",
        "    board_size: defines the size of the board (the board is square)\n",
        "    --------------------------------------------------\n",
        "    qvalues: dictionary of the q-values where the keys are the states and\n",
        "              the values are the q-values for each of the actions\n",
        "    results: list of result dictionaries which have 'game_count' and 'score' keys\n",
        "    history: list of dictionaries which have 'state' and 'action' keys\n",
        "              which correspond to states and actions performed in those states\n",
        "    board_history: list of numpy arrays of the board state, these can be drawn using pyplot\n",
        "    animation: matplotlib.animation.FuncAnimation object with the animation of the whole game\n",
        "    '''\n",
        "\n",
        "    self.board_size = board_size\n",
        "\n",
        "    self.restart()\n",
        "    self.qvalues = {}\n",
        "\n",
        "    self.results = []\n",
        "\n",
        "  def train_qvalues(self, epsilon=0.1, lr=0.7, discount=0.5,\n",
        "                    rewards = [10, 5, -10, -2], remove_epsilon=100, starvation=50,\n",
        "                    max_games=200, update_whole_history=False, verbose=True):\n",
        "    '''\n",
        "    train the q-values\n",
        "    epsilon: epsilon for the epsilon-greedy algorithm\n",
        "    lr: learning rate (alpha) for the Q-learning algorithm\n",
        "    discount: gamma parameter in the Bellman equation\n",
        "    rewards: a list of rewards in the following order: \\\n",
        "            [reward for eating an apple, reward for getting closer to the apple \\\n",
        "            reward for dying (negative), reward for getting further away from the apple(negative)]\n",
        "    remove_epsilon: number of games after which the epsilon should be set to equal 0\n",
        "    starvation: maximum amount of time without eating an apple before dying of starvation\n",
        "    max_games: number of games (episodes) to train for\n",
        "    update_whole_history: if True, updates the whole history of q-values at each iteration\n",
        "    verbose: if True, print the result of each game\n",
        "    '''\n",
        "    self.lr = lr\n",
        "    self.discount = discount\n",
        "    self.apple_reward, self.closer_reward, self.death_reward, self.further_reward = rewards\n",
        "    self.remove_epsilon = remove_epsilon\n",
        "    self.starvation = starvation\n",
        "    self.update_whole_history = update_whole_history\n",
        "\n",
        "    for game_count in tqdm(range(1, max_games + 1), position=0, leave=True):\n",
        "      ## restart the game\n",
        "      self.restart()\n",
        "      ## if the game_count is high enough, make epsilon = 0\n",
        "      if game_count > self.remove_epsilon:\n",
        "        self.epsilon = 0\n",
        "      else:\n",
        "        self.epsilon = epsilon\n",
        "      ## play until dead\n",
        "      self.play_game(train=True)\n",
        "\n",
        "      ## generate and store the results of the game for future reference\n",
        "      results = {\n",
        "          'game_count': game_count,\n",
        "          'score': self.game.score\n",
        "      }\n",
        "      self.results.append(results)\n",
        "\n",
        "      if verbose:\n",
        "        print(results)\n",
        "\n",
        "  def play_multiple(self, games=40):\n",
        "    '''\n",
        "    play multiple games and return the results of each of the games\n",
        "    games: number of games to play\n",
        "    '''\n",
        "    scores = []\n",
        "    for _ in tqdm(range(games)):\n",
        "      self.restart()\n",
        "      self.play_game(train=False)\n",
        "      scores.append(self.game.score)\n",
        "    return np.array(scores)\n",
        "\n",
        "  def play_game(self, train=True):\n",
        "    '''\n",
        "    play a game until the snake dies\n",
        "    train: if True, trains the q-values as well\n",
        "    '''\n",
        "    idle = 0\n",
        "    while not self.dead:\n",
        "      idle += 1\n",
        "      ## get an action and perform the action\n",
        "      action, state = self.get_action()\n",
        "      if state.ate:\n",
        "        idle = 0\n",
        "      self.perform_action(action)\n",
        "      if idle >= self.starvation:\n",
        "        self.dead = True\n",
        "      ## if the q-values are being trained, update them\n",
        "      if train:\n",
        "        self.update_qvalues(self.update_whole_history)\n",
        "\n",
        "  def play_dummy(self):\n",
        "    self.restart()\n",
        "    actions = [Input.CHILL, Input.TURN_RIGHT, Input.CHILL, Input.TURN_LEFT, Input.CHILL]\n",
        "    for action in actions:\n",
        "      self.perform_action(action)\n",
        "\n",
        "  def get_qvalues(self, state):\n",
        "    '''get the q-values for a given state'''\n",
        "    if state not in self.qvalues:\n",
        "      self.qvalues[state] = np.zeros(3)\n",
        "\n",
        "    return self.qvalues[state]\n",
        "\n",
        "  def print_qvalues(self):\n",
        "    '''get a pandas DataFrame of the q-values in a human readable format'''\n",
        "    return pd.DataFrame.from_dict(self.qvalues, orient='index')\n",
        "\n",
        "  def restart(self):\n",
        "    '''\n",
        "    restart the game\n",
        "    '''\n",
        "    self.game = SnakeGame(self.board_size)\n",
        "    self.dead = False\n",
        "    self.history = []\n",
        "\n",
        "    self.board_history = []\n",
        "    self.board_history.append(self.game.draw())\n",
        "\n",
        "  def get_action(self):\n",
        "    '''\n",
        "    get the action for the current state of the game using the epsilon greedy algorithm\n",
        "    if self.epsilon=0, returns the action with the highest q-value\n",
        "    also stores the action and the state into self.history list\n",
        "    '''\n",
        "    state = self.game.get_state()\n",
        "\n",
        "    ## epsilon greedy\n",
        "    rand = random.uniform(0, 1)\n",
        "    if rand < self.epsilon:\n",
        "        action = random.choice(list(Input))\n",
        "    else:\n",
        "        state_scores = self.get_qvalues(state.get_state_str())\n",
        "        action = Input(state_scores.argmax())\n",
        "\n",
        "    ## Remember the actions it took at each state\n",
        "    self.history.append({\n",
        "        'state': state,\n",
        "        'action': action\n",
        "        })\n",
        "    return action, state\n",
        "\n",
        "  def perform_action(self, input):\n",
        "    '''\n",
        "    if the snake is not dead, perform the input action\n",
        "    if the snake dies, set the score and prevent further play\n",
        "    also always add the board state array to the list\n",
        "    input: action the snake should take, Input object\n",
        "    '''\n",
        "    if not self.dead:\n",
        "      self.game.input(input)\n",
        "      ## if the snake died\n",
        "      if self.game.dead:\n",
        "        self.score = self.game.score\n",
        "        self.dead = True\n",
        "      self.board_history.append(self.game.draw())\n",
        "    return\n",
        "\n",
        "  def update_qvalue(self, state, action, future_state=None, death=False):\n",
        "    '''\n",
        "    update the q-values using the Bellman equation\n",
        "    if the snake has died, the last state-action q-value will be set\n",
        "    to equal the self.death_reward value\n",
        "    state: state of the last step\n",
        "    action: action of the last step\n",
        "    future_state: state after the action of the last step\n",
        "    death: bool value, true if the snake has died\n",
        "    '''\n",
        "    state_str = state.get_state_str()\n",
        "    action_value = action.value\n",
        "    if death:\n",
        "      ## TODO Implement the q-value update on death\n",
        "      ## q-value is equal to the death reward\n",
        "      ## there is no future state since game is over\n",
        "      self.get_qvalues(state_str)[action_value] = 0\n",
        "    else:\n",
        "      if future_state.ate: # Snake ate a food, positive reward\n",
        "        reward = self.apple_reward\n",
        "      elif future_state.distance < state.distance: # Snake is closer to the food, positive reward\n",
        "        reward = self.closer_reward\n",
        "      else:\n",
        "        reward = self.further_reward # Snake is further from the food, negative reward\n",
        "\n",
        "      future_state_str = future_state.get_state_str()\n",
        "\n",
        "      ## TODO Implement the Bellman equation\n",
        "      ## alpha - self.lr\n",
        "      ## gamma - self.discount\n",
        "      ## self.get_qvalues(state_string_) - gets the numpy array for the given state string\n",
        "      self.get_qvalues(state_str)[action_value] = 0 # Bellman equation\n",
        "\n",
        "  def update_qvalues(self, update_whole_history):\n",
        "    if update_whole_history:\n",
        "      self.update_qvalues_whole_history()\n",
        "    else:\n",
        "      self.update_current_qvalue()\n",
        "\n",
        "  def update_current_qvalue(self):\n",
        "    current_state = self.game.get_state() # current state\n",
        "    previous_state = self.history[-1]['state'] # previous state\n",
        "    previous_action = self.history[-1]['action'] # action taken at previous state\n",
        "\n",
        "    self.update_qvalue(previous_state, previous_action, current_state, self.game.dead)\n",
        "\n",
        "  def update_qvalues_whole_history(self):\n",
        "    '''\n",
        "    update the q-values for all the state-action pairs in the current episode\n",
        "    death: bool value, true if the snake has died\n",
        "    '''\n",
        "\n",
        "    ## reverse history, 0th element is the last step\n",
        "    history = self.history[::-1]\n",
        "    death = self.dead\n",
        "    for i, h in enumerate(history[:-1]):\n",
        "      if death: # Snake Died\n",
        "        state = history[0]['state']\n",
        "        action = history[0]['action']\n",
        "\n",
        "        self.update_qvalue(state, action, death=death)\n",
        "        death = False\n",
        "\n",
        "      else:\n",
        "        current_state = h['state'] # current state\n",
        "        previous_state = history[i+1]['state'] # previous state\n",
        "        previous_action = history[i+1]['action'] # action taken at previous state\n",
        "\n",
        "        self.update_qvalue(previous_state, previous_action, current_state)\n",
        "\n",
        "  def draw_history(self, save_path = None):\n",
        "    '''\n",
        "    draw the whole board_history as images and saves the animation\n",
        "    save_path: if not None, saves the animation as a gif with the given path\n",
        "    '''\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(self.board_history[0])\n",
        "    ax.tick_params(\n",
        "      axis='both',\n",
        "      which='both',\n",
        "      bottom=False,\n",
        "      top=False,\n",
        "      left=False,\n",
        "      right=False,\n",
        "      labelbottom=False,\n",
        "      labelleft=False)\n",
        "    def update(i):\n",
        "      im.set_array(self.board_history[i])\n",
        "      return im,\n",
        "    animation_fig = plt_animation.FuncAnimation(fig, update, frames=len(self.board_history), interval=50, blit=True)\n",
        "\n",
        "    if save_path is not None:\n",
        "      animation_fig.save(save_path)\n",
        "\n",
        "    self.animation = animation_fig"
      ],
      "metadata": {
        "id": "T3q-u5ydFyKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zadaci\n",
        "\n",
        "1. Proučite klasu `Snake` i pomoćne klase, isprobajte rad klase kreiranjem objekta iste i odigrajte nekoliko poteza pomoću funkcije `input`. Između poteza ispišite stanje i iscrtajte polje za igru.\n",
        "\n",
        "2. Proučite klasu `Play`. Sve funkcije su opisane u kodu. Isprobajte funkcije `play_dummy` i `draw_history` kako biste dobili animaciju primjera igranja. Animaciju možete vizualizirati pristupanjem `animation` atributu klase `Play`. Također možete spremiti animaciju kao gif predajom putanje funkciji `draw_history`.\n",
        "\n",
        "3. Implementirajte ažuriranje q-vrijednosti u funkciji `update_qvalue`. Pokrenite *Q-learning* algoritam i istrenirajte q-vrijednosti pomoću funkcije `train_qvalues`. Proučite i odaberite prikladne parametre. Vizualizirajte posljednju epizodu igre.\n",
        "\n",
        "4. Korištenjem funkcije `play_multiple` odredite prosječni rezultat nakon treniranja.\n",
        "\n",
        "5. Dodajte funkcije za spremanje i učitavanje q-vrijednosti. Ponovno istrenirajte q-vrijednosti uz spremanje q-vrijednosti nakon svake epizode. Zatim učitajte q-vrijednosti nakon svake epizode i odredite prosječni rezultat za svaku epizodu treniranja. Prikažite dobivene rezultate na grafu.\n",
        "\n",
        "6. Ponovite 5. zadatak s različitim parametrima treniranja. Usporedite dobivene rezultate.\n",
        "\n",
        "7. Izmijenite klasu `State` i funkciju `get_state` klase `SnakeGame` kako biste obuhvatili više informaciju u stanju igre. Prilagodite i druge potrebne funkcije i klase te istrenirajte q-vrijednosti s novom definicijom stanja. Ponovite 5. zadatak za ovu definiciju stanja.\n",
        "\n",
        "Dodatno: Dodajte tekst za praćenje rezultata u animaciju igre. Dodajte vizualizaciju stanja igre u svakom trenutku u animaciju igre."
      ],
      "metadata": {
        "id": "uSHpDOD9zF7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "play = Play(10)"
      ],
      "metadata": {
        "id": "uC9Zz6pDAHR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play.play_dummy()"
      ],
      "metadata": {
        "id": "tCMSrlpncxWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "## prevent the output of the cell since the plt.imshow()\n",
        "## shows the image automatically\n",
        "play.draw_history('animation.gif')"
      ],
      "metadata": {
        "id": "bEjR1LcqRYkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play.animation"
      ],
      "metadata": {
        "id": "H7fH7WMlZuUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5SnMjDksGjlO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}